{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b362f98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import ElasticNet\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ddef3e",
   "metadata": {},
   "source": [
    "### Data Source: https://archive.ics.uci.edu/ml/datasets/Auto+MPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd9186d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# reading the dataset\n",
    "\n",
    "df = pd.read_csv('data/auto-mpg.data', sep='\\n', header=None)\n",
    "\n",
    "df[[0, 'car_name']] = df[0].str.split('\\t', expand=True)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fb721a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# defining column names\n",
    "columns = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model_year', 'origin']\n",
    "\n",
    "# refining the dataframe\n",
    "df[columns] = df[0].str.split(expand=True)\n",
    "df.drop(columns=[0], inplace=True)\n",
    "df['car_name'] = df['car_name'].apply(lambda x: x.replace('\"', ''))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58bd884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting columns to float type\n",
    "for col in df.columns:\n",
    "    if col not in ['mpg', 'car_name']:\n",
    "        df = df[pd.to_numeric(df[col], errors='coerce').notnull()]\n",
    "        df[col] = df[col].astype(float)\n",
    "\n",
    "# seperating dependant and independant variables\n",
    "X = df[['cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model_year', 'origin']]\n",
    "y = df['mpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18dbd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "train_X, test_X, train_y, test_y = train_test_split(X,y,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e3c06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to evaluate model performance\n",
    "def eval_metrics(actual, pred):\n",
    "    rmse = np.sqrt(mean_squared_error(actual, pred))\n",
    "    mae = mean_absolute_error(actual, pred)\n",
    "    return rmse, mae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfef3f2",
   "metadata": {},
   "source": [
    "## The conventional way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6260281",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha, l1_ratio = 0.01, 0.15\n",
    "    \n",
    "# initiating an elastic net model\n",
    "lr = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)\n",
    "\n",
    "# fitting the model with train dataset\n",
    "lr.fit(train_X, train_y)\n",
    "\n",
    "# making predictions on test set\n",
    "y_pred = lr.predict(test_X)\n",
    "\n",
    "# obtaining the model performance\n",
    "rmse, mae = eval_metrics(test_y, y_pred)\n",
    "\n",
    "print('Hyperparameters: Alpha =  {}, L1 Ratio = {} \\n'.format(alpha, l1_ratio))\n",
    "\n",
    "print('Model Performance on test set: RMSE = {}, MAE = {} \\n'.format(rmse, mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef30d18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas, l1_ratios = [0.01, 0.02, 0.5], [0.15, 0.2, 0.5]\n",
    "    \n",
    "for alpha in alphas:\n",
    "    for l1_ratio in l1_ratios:\n",
    "        # initiating an elastic net model\n",
    "        lr = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)\n",
    "\n",
    "        # fitting the model with train dataset\n",
    "        lr.fit(train_X, train_y)\n",
    "\n",
    "        # making predictions on test set\n",
    "        y_pred = lr.predict(test_X)\n",
    "\n",
    "        # obtaining the model performance\n",
    "        rmse, mae = eval_metrics(test_y, y_pred)\n",
    "\n",
    "        print('Hyperparameters: Alpha =  {}, L1 Ratio = {} \\n'.format(alpha, l1_ratio))\n",
    "\n",
    "        print('Model Performance on test set: RMSE = {}, MAE = {} \\n'.format(rmse, mae))\n",
    "        \n",
    "        print ('-'*50,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32879eb1",
   "metadata": {},
   "source": [
    "## Using mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb502b85",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# defining a new experiment\n",
    "experiment_name = 'PlainRegression'\n",
    "# returns experiment ID\n",
    "try:\n",
    "    # creating a new experiment\n",
    "    exp_id = mlflow.create_experiment(name=experiment_name)\n",
    "except Exception as e:\n",
    "    exp_id = mlflow.get_experiment_by_name(experiment_name).experiment_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bc9b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'images' not in os.listdir():\n",
    "    os.mkdir('images')\n",
    "\n",
    "with mlflow.start_run(experiment_id=exp_id):\n",
    "    \n",
    "    # simulating EDA process by creating distribution plots for all the features\n",
    "    train_X.plot(kind='box', subplots=True, layout=(2,4), figsize=(16,9), title='Box plot of each feature')\n",
    "    \n",
    "    # saving the image to images folder\n",
    "    plt.savefig('images/distribution_plot_all_features.png')\n",
    "\n",
    "    # logging artifacts -> saves the image and enables tracking for later use\n",
    "    mlflow.log_artifacts('images')\n",
    "    \n",
    "    # defining alpha and l1 ratio\n",
    "    alpha, l1_ratio = 0.02, 0.15\n",
    "    \n",
    "    # initiating an elastic net model\n",
    "    lr = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)\n",
    "\n",
    "    # fitting the model with train dataset\n",
    "    lr.fit(train_X, train_y)\n",
    "\n",
    "    # making predictions on test set\n",
    "    y_pred = lr.predict(test_X)\n",
    "\n",
    "    # obtaining the model performance\n",
    "    rmse, mae = eval_metrics(test_y, y_pred)\n",
    "    \n",
    "    # logging the parameters \n",
    "    mlflow.log_param('alpha', alpha)\n",
    "    mlflow.log_param('l1_ratio', l1_ratio)\n",
    "    \n",
    "    # logging the metrics\n",
    "    mlflow.log_metric('rmse', rmse)\n",
    "    mlflow.log_metric('mae', mae)\n",
    "    \n",
    "    # saving the model for later use\n",
    "    mlflow.sklearn.log_model(lr, \"PlainRegression_Model\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd83d44",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning using mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfadd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a new experiment\n",
    "experiment_name = 'PlainRegression_HyperParameter_Search'\n",
    "# returns experiment ID\n",
    "try:\n",
    "    # creating a new experiment\n",
    "    exp_id = mlflow.create_experiment(name=experiment_name)\n",
    "except Exception as e:\n",
    "    exp_id = mlflow.get_experiment_by_name(experiment_name).experiment_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6003f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining alpha and l1 ratio\n",
    "alphas, l1_ratios = [0.01, 0.05, 0.1, 0.02, 0.03], [0.15, 0.1, 0.2, 0.25]\n",
    "\n",
    "for alpha in alphas:\n",
    "    for l1_ratio in l1_ratios:\n",
    "        # starting an mlflow run, and tracking them under the experiment defined above\n",
    "        with mlflow.start_run(experiment_id=exp_id):\n",
    "\n",
    "            # logging artifacts -> saves the image and enables tracking for later use\n",
    "            mlflow.log_artifacts('images')\n",
    "\n",
    "            # initiating an elastic net model\n",
    "            lr = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)\n",
    "\n",
    "            # fitting the model with train dataset\n",
    "            lr.fit(train_X, train_y)\n",
    "\n",
    "            # making predictions on test set\n",
    "            y_pred = lr.predict(test_X)\n",
    "\n",
    "            # obtaining the model performance\n",
    "            rmse, mae = eval_metrics(test_y, y_pred)\n",
    "\n",
    "            # logging hyperparameters defined above\n",
    "            mlflow.log_param(\"alpha\", alpha)\n",
    "            mlflow.log_param(\"l1_ratio\", l1_ratio)\n",
    "\n",
    "            # logging performance of the model\n",
    "            mlflow.log_metric(\"rmse\", rmse)\n",
    "            mlflow.log_metric(\"mae\", mae)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47831447",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003c1a44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
